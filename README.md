<!--
---
title: "Epstein Files ARD"
description: "Analysis Ready Dataset applying the ARD layer model to DOJ-released Epstein investigation documents"
author: "VintageDon"
orcid: "0009-0008-7695-4093"
date: "2026-02-01"
version: "0.3"
status: "Active"
tags:
  - type: project-root
  - domain: [ard, investigative-journalism, osint]
  - tech: [python, postgresql, pgvector, spacy, rag]
related_documents:
  - "[ARD Methodology](https://github.com/vintagedon/analysis-ready-dataset)"
  - "[DOJ Epstein Library](https://www.justice.gov/epstein)"
---
-->

# ğŸ—‚ï¸ Epstein Files ARD

[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python&logoColor=white)](https://python.org)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-pgvector-4169E1?logo=postgresql&logoColor=white)](https://github.com/pgvector/pgvector)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)
[![Data License](https://img.shields.io/badge/Data-CC--BY--4.0-green)](LICENSE-DATA)

![Repository Banner](assets/repo-banner.jpg)

> An Analysis Ready Dataset transforming DOJ Epstein investigation files into a queryable, enriched corpus with pre-computed entity resolution, document classification, and relationship graphs.

This project applies the [ARD layer model](https://github.com/vintagedon/analysis-ready-dataset) to publicly released documents from the Jeffrey Epstein investigation. Rather than distributing raw scans and expecting each researcher to independently extract entities, resolve names, and build relationship maps, this ARD front-loads that computational workâ€”performing expensive operations once with rigor so others don't have to repeat them.

---

## âš ï¸ Responsible Use

This project processes sensitive documents related to serious crimes. We are committed to ethical data handling:

Victim Protection: We anonymize victim-identifying information in all processed outputs. While unredacted source materials exist publicly, we choose not to perpetuate the exposure of individuals who did not consent to public identification. Researchers using this dataset should maintain the same standard.

What This Means in Practice:

- Victim names, when identifiable, are replaced with anonymized identifiers
- We do not provide tools or guidance for de-anonymizing protected individuals
- Entity resolution focuses on subjects of investigation, not victims
- We acknowledge that determined actors could cross-reference with public sourcesâ€”we cannot prevent this, but we will not facilitate it

This is a conscious choice to prioritize victim dignity over dataset completeness.

---

## ğŸ”­ Overview

This section explains the ARD approach and project motivation. If you're ready to use the data, skip to [Quick Start](#-quick-start).

The January 2026 DOJ release included over 3 million pages of documents, 180,000 images, and 2,000 videos. These materials are scattered across formats, riddled with OCR noise, and lack structure. Every journalist or researcher working this corpus independently repeats the same foundational processing: extracting entities, resolving name variants, classifying document types, and building relationship networks.

This project absorbs that compute debt. By building an Analysis Ready Dataset, we convert processor time into storage spaceâ€”pre-computing the derived quantities that appear repeatedly in investigative work but require significant effort to generate.

This is also a learning project. The ARD is being built alongside the [IBM RAG and Agentic AI Professional Certificate](https://www.coursera.org/professional-certificates/ibm-rag-and-agentic-ai), applying each technique to real documents rather than toy datasets.

---

## ğŸ¯ Target Audience

| Audience | Use Case |
|----------|----------|
| Investigative Journalists | Query pre-computed relationships without building NER pipelines |
| OSINT Researchers | Similarity search, entity graphs, cross-document analysis |
| Data Scientists | RAG system development against a real-world messy corpus |
| ARD Implementers | Third case study validating the methodology across domains |

---

## ğŸ“Š Project Status

| Area | Status | Description |
|------|--------|-------------|
| Repository Setup | âœ… Complete | M01: Scaffolding, memory bank, scope definition |
| GitHub Project | âœ… Complete | M02: Milestones, labels, 18 tasks configured |
| Source Evaluation | âœ… Complete | M03: Source selection, L0 import with provenance |
| Layer 0: Canonical | ğŸ”„ In Progress | M04: Schema validation, quality audit |
| Layer 1: Scalars | â¬œ Planned | M05: Entity extraction, classification |
| Layer 2: Vectors | â¬œ Planned | M06: Embeddings, similarity search |
| Layer 3: Graphs | â¬œ Planned | M07: Entity resolution, relationships |
| Web Interface | â¬œ Planned | M08: Public search at epsteinfiles.dev |

---

## ğŸ“‚ Data Sources

This project uses two primary sources for the bounded starter corpus:

| Source | Dataset | Records | Provenance |
|--------|---------|---------|------------|
| [Internet Archive](https://archive.org/details/epstein-flight-logs-unredacted_202304) | Flight Logs | 5,001 | Bradley Edwards court exhibits (Epstein v. Edwards) |
| [epsteinsblackbook.com](https://epsteinsblackbook.com/files) | Black Book | 2,324 | Wayback Machine archived scans |

Full provenance documentation including SHA-256 hashes is available in [research/source-analysis/](research/source-analysis/).

### Why These Sources?

During M03, we evaluated multiple existing datasets:

| Source | Assessment | Decision |
|--------|------------|----------|
| Internet Archive flight logs PDF | Pre-structured 22-column format, 82% passenger identification | âœ… Selected |
| epsteinsblackbook.com black book CSV | Row-level Wayback provenance, parsed fields | âœ… Selected |
| epsteinsblackbook.com flight CSV | OCR artifacts, only 3 columns | âŒ Rejected |
| theelderemo/FULL_EPSTEIN_INDEX | Quality work, but 32K documents exceeds bounded scope | â¸ï¸ Deferred |

The ARD methodology assumes you inherit from whatever layer exists. We selected the highest-quality existing extractions and will add value through Layers 1-3.

---

## ğŸ—ï¸ Architecture

### The ARD Layer Model

Each layer builds on the one below, enabling progressively richer query patterns:

![ARD Layer Model](assets/ard-layer-model-infographic.jpg)

### Initial Scope: Bounded Starter Corpus

| Document Set | Records | Structure | Value |
|--------------|---------|-----------|-------|
| Flight Logs | 5,001 | Tabular (date, route, passengers) | Temporal patterns, co-occurrence |
| Black Book | 2,324 | Contact directory (names, phones, addresses) | Entity baseline, relationship seeds |

These two datasets are highly structured, naturally joinable, and provide concrete deliverables while the methodology matures.

### Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Processing | Python 3.11+, pandas, spaCy | Ingestion, NER, transformation |
| Storage | PostgreSQL 16+ | Primary data store |
| Vector Search | pgvector | Similarity queries |
| Embeddings | sentence-transformers | Text vectorization |

---

## ğŸ“ Repository Structure

```
epsteinfiles-dev/
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ raw/                    # Original files (gitignored)
â”‚   â”œâ”€â”€ layer-0-canonical/      # Cleaned, schema'd outputs
â”‚   â”œâ”€â”€ layer-1-scalars/        # Derived quantities
â”‚   â”œâ”€â”€ layer-2-vectors/        # Embeddings
â”‚   â””â”€â”€ layer-3-graphs/         # Relationship structures
â”œâ”€â”€ ğŸ“‚ pipelines/
â”‚   â”œâ”€â”€ ingestion/              # Acquisition scripts
â”‚   â”œâ”€â”€ processing/             # Layer transformations
â”‚   â””â”€â”€ validation/             # Quality checks
â”œâ”€â”€ ğŸ“‚ scripts/                 # Automation and setup
â”œâ”€â”€ ğŸ“‚ notebooks/               # Exploratory analysis
â”œâ”€â”€ ğŸ“‚ src/                     # Production code (API, web)
â”œâ”€â”€ ğŸ“‚ docs/
â”‚   â”œâ”€â”€ case-study/             # ARD methodology documentation
â”‚   â”œâ”€â”€ ethics/                 # Data handling guidelines
â”‚   â””â”€â”€ documentation-standards/
â”œâ”€â”€ ğŸ“‚ research/                # Source analysis, GDR artifacts
â”œâ”€â”€ ğŸ“‚ work-logs/               # Development history
â”œâ”€â”€ ğŸ“„ LICENSE                  # MIT (code)
â”œâ”€â”€ ğŸ“„ LICENSE-DATA             # CC-BY-4.0 (processed data)
â””â”€â”€ ğŸ“„ README.md                # This file
```

---

## âš–ï¸ Ethical Framework

This project operates under strict ethical guidelines:

| Principle | Implementation |
|-----------|----------------|
| Respect Redactions | Honor all redactions in source documents |
| Protect Victims | Anonymize victim-identifying information (see [Responsible Use](#-responsible-use)) |
| No Fine-Tuning | Do not train generative models on this corpus |
| No Commercial Use | Public interest and educational purposes only |
| Verify Facts | Present RAG outputs as leads, not established truth |
| Provenance | Every processed record links to its source |

These guidelines align with community standards established by existing Epstein file projects.

---

## ğŸ”¬ Related Work

| Project | Description | Relationship |
|---------|-------------|--------------|
| [ARD Methodology](https://github.com/vintagedon/analysis-ready-dataset) | Layer model framework | This is the third case study |
| [theelderemo/FULL_EPSTEIN_INDEX](https://github.com/theelderemo/FULL_EPSTEIN_INDEX) | Comprehensive archive with HuggingFace dataset | Reference; scope exceeds our bounded corpus |
| [epsteinsblackbook.com](https://epsteinsblackbook.com) | Searchable archive with structured downloads | Black Book source |
| [DOJ Epstein Library](https://www.justice.gov/epstein) | Official releases | Primary provenance anchor |

---

## ğŸŒŸ Open Science Philosophy

We practice open science and open methodology:

- Research processes are documented as they happen, not reconstructed after
- Methodologies are published so others can adapt them to other corpora
- Learning processes are capturedâ€”this project exists partly to learn RAG techniques
- Limitations are stated clearly; we don't oversell what the data can tell you

---

## ğŸš€ Quick Start

### For Researchers (Using the Data)

*Data products not yet available. Check back after Layer 1 completion.*

### For Contributors (Building the ARD)

```bash
# Clone repository
git clone https://github.com/vintagedon/epsteinfiles-dev.git
cd epsteinfiles-dev

# Create Python environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install pandas polars pyarrow pdfplumber spacy sentence-transformers psycopg2-binary pgvector

# Download spaCy model
python -m spacy download en_core_web_lg
```

See [tech.md](.kilocode/rules/memory-bank/tech.md) for full environment setup.

---

## ğŸ“„ License

- Code: [MIT License](LICENSE)
- Processed Data: [CC-BY-4.0](LICENSE-DATA)
- Source Documents: Public government records released under the Epstein Files Transparency Act

---

## ğŸ™ Acknowledgments

- DOJ and House Oversight Committee â€” Source document releases
- Existing consolidation projects â€” Foundation work on OCR and initial processing
- ARD methodology â€” Framework for structured dataset enhancement
- IBM / Coursera â€” RAG and Agentic AI certification providing learning structure

---

Last Updated: 2026-02-01 | Status: M04 Layer 0 Validation
